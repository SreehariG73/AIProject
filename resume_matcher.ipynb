{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# if there are no conflicting packages in the default Python Libs =>\n",
    "sys.path.append(\"/usr/home/username/pdfminer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "import os\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.util import minibatch\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm') \n",
    "ner = nlp.get_pipe(\"ner\")  # Access the NER component\n",
    "\n",
    "for label in [\"SKILL\", \"EDUCATION\", \"EXPERIENCE\", ...]:\n",
    "    ner.add_label(label)  # Add your custom labels\n",
    "\n",
    "optimizer = nlp.initialize()  \n",
    "\n",
    "for itn in range(100):  # Adjust number of iterations\n",
    "    losses = {}\n",
    "    batches = minibatch(training_data, size=8) # Adjust batch size\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        nlp.update(texts, annotations, sgd=optimizer, losses=losses)\n",
    "nlp.to_disk(\"resume_parser_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        parser = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(parser.pages)):\n",
    "            page = parser.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def process_resume(doc):\n",
    "    skills = [ent.text for ent in doc.ents if ent.label_ in [\"SKILLS\",\"Skills\",\"skills\"]]\n",
    "    experience = [ent.text for ent in doc.ents if ent.label_ in [\"EXPERIENCE\",\"Experience\",\"experience\"]]\n",
    "    # ... extract other relevant sections\n",
    "    return {\"skills\": skills, \"experience\": experience} \n",
    "\n",
    "def calculate_similarity(resume_data, job_desc):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    resume_text = \" \".join([\n",
    "        \" \".join(resume_data[\"skills\"]),\n",
    "        \" \".join(resume_data[\"experience\"])\n",
    "    ])\n",
    "    tfidf_matrix = vectorizer.fit_transform([resume_text, job_desc])\n",
    "    return cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'resume_parser_model'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/x4/sd3bc1s15jxdbxm6r5jkp6sr0000gn/T/ipykernel_1211/3103560078.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# -- Main Logic --\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"resume_parser_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mjob_description\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Information technology specialist\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresume_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Users/sreehariguruprasad/myprojects/AI Project/resume dataset/data/data/INFORMATION-TECHNOLOGY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \"\"\"\n\u001b[0;32m---> 51\u001b[0;31m     return util.load_model(\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[index]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'resume_parser_model'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "# -- Main Logic --\n",
    "nlp = spacy.load(\"resume_parser_model\") \n",
    "job_description = \"Information technology specialist\"\n",
    "\n",
    "resume_folder = \"/Users/sreehariguruprasad/myprojects/AI Project/resume dataset/data/data/INFORMATION-TECHNOLOGY\"\n",
    "all_resumes = [f for f in os.listdir(resume_folder) if f.endswith(\".pdf\")]\n",
    "random.shuffle(all_resumes)  # Shuffle for random selection\n",
    "\n",
    "test_size = 0.2  # 20% of the dataset for testing\n",
    "test_set_count = int(len(all_resumes) * test_size)\n",
    "test_set = all_resumes[:test_set_count]\n",
    "training_set = all_resumes[test_set_count:]\n",
    "\n",
    "# -- Training --\n",
    "selected_resumes = random.sample(training_set, 10)\n",
    "\n",
    "for resume_file in selected_resumes:\n",
    "    file_path = os.path.join(resume_folder, resume_file)\n",
    "    text = extract_text_from_pdf(file_path)\n",
    "    doc = nlp(text)\n",
    "    data = process_resume(doc)\n",
    "    similarity = calculate_similarity(data, job_description)\n",
    "    print(f\"{resume_file} - Similarity: {similarity:.2f}\")\n",
    "\n",
    "# Suggest the resume with the highest similarity\n",
    "\n",
    "# -- Testing --\n",
    "for resume_file in test_set:\n",
    "    file_path = os.path.join(resume_folder, resume_file)\n",
    "    text = extract_text_from_pdf(file_path)\n",
    "    doc = nlp(text)\n",
    "    data = process_resume(doc)\n",
    "    similarity = calculate_similarity(data, job_description)\n",
    "    print(f\"[Test] {resume_file} - Similarity: {similarity:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
